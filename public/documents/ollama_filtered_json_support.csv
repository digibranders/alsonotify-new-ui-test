Model Name,Parameters,File Size,Context Window,CPU Speed (tok/s),GPU Speed (tok/s),Min RAM (Host),Latency (P50 ms),JSON Support,Use Case,Notes
Qwen2.5 (3B),3B,1.8-2.0 GB,32K,~1.5-2.0,45-55,6 GB,200-350,Yes (Structured Output),"Multilingual, extended context","Excellent JSON schema support, recommended for production"
Qwen 3 (4B),4B,2.4-2.7 GB,32K,~1.8-2.5,40-50,7-8 GB,250-380,Yes (Structured Output),"Advanced reasoning, JSON extraction","Native structured output support, tool calling capable"
Qwen 3 (8B),8B,4.7-5.2 GB,32K,~2.5-3.5,35-45,8 GB,320-450,Yes (Structured Output + Thinking mode),"Complex tasks, reasoning, data extraction","Thinking mode enabled, excellent for structured tasks"
Gemma 3 (4B),4B,3.3 GB,128K,~2.0-3.0,40-50,7-8 GB,280-450,"Yes (Structured Output, Multimodal)","Long document processing, vision tasks","128K context with vision, excellent for RAG, multimodal support"
Phi-3 Mini (3.8B),3.8B,2.2-2.4 GB,128K,~1.5-2.0,45-55,7-8 GB,250-400,Yes (Structured Output + Function Calling),"Reasoning-heavy tasks, function calling, RAG","128K context, excellent reasoning, strong function calling"
Phi-4 Mini (3.8B),3.8B,2.2-2.5 GB,4K,~1.5-2.0,48-60,7-8 GB,200-350,Yes (Structured Output + Function Calling),"Function calling, multilingual tasks","Native tool calling, best-in-class function support"
Granite 3 (2B),2B,1.2-1.4 GB,128K,~1.2-1.8,52-65,5-6 GB,150-280,Yes (Structured Output + Tool Calling),"Long document RAG, tool-based tasks","128K context, IBM model, tool calling capable"
Granite 3.3 (2B),2B,1.2-1.4 GB,128K,~1.2-1.8,50-65,5-6 GB,160-290,Yes (Structured Output + Function Calling),"Improved reasoning, function calling, RAG","Latest Granite 3.3, improved reasoning and tool calling"
Granite 3.3 (8B),8B,4.7-5.2 GB,128K,~1.8-2.5,32-42,8 GB,350-500,Yes (Structured Output + Function Calling),"Enterprise RAG, complex tool-calling workflows","128K context, fits in 8GB tight, strong function calling"
Llama 3.2 (3B),3B,1.9-2.1 GB,8K,~1.5-2.0,48-60,6 GB,180-300,Yes (Structured Output),"General purpose, JSON extraction","Meta model, reliable structured output, proven JSON support"
DeepSeek-R1 (1.5B),1.5B,0.9-1.1 GB,32K,~1.0-1.5,55-70,5 GB,140-250,Partial (use json_schema mode),"Reasoning-focused tasks, thinking through problems","Use json_schema method mode, returns reasoning + output in JSON"
DeepSeek-R1 (7B),7B,4.1-4.5 GB,32K,~2.5-3.5,35-45,8 GB,300-450,Partial (use json_schema mode),"Complex reasoning, structured analysis","Strong reasoning, json_schema mode with thinking output"
DeepSeek-R1 (8B),8B,4.7-5.3 GB,32K,~2.8-3.8,32-42,8 GB,320-480,Partial (use json_schema mode),"Advanced reasoning with thinking, data extraction","Ollama integrated thinking mode, json_schema method recommended"
Mistral (7B),7B,4.1-4.5 GB,32K,~2.0-3.0,38-48,8 GB,280-420,Yes (Structured Output),"Fast inference, general JSON extraction","Reliable baseline, good structured output support"
Falcon 3 (7B),7B,4.0-4.5 GB,8K,~2.0-3.0,35-45,8 GB,350-500,Yes (Structured Output),"Coding tasks, technical JSON schemas","Coding-optimized, structured output support available"
Granite 4 (3B),3B,1.8-2.0 GB,4K,~1.5-2.0,45-55,6 GB,200-350,Yes (Structured Output + Tool Calling),"Tool-calling workflows, instruction following","Latest Granite 4, tool calling and function support"
NemoTron Mini (4B),4B,2.4-2.7 GB,4K,~1.8-2.3,40-50,7-8 GB,240-380,Yes (Function Calling + Structured),"NVIDIA-optimized RAG, function calling","NVIDIA model, RAG-optimized, strong function calling"
